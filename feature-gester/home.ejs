<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>AirBoard</title>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>
    <script src="fingerpose.js" type="text/javascript"></script>
    <style>
      #video-container {
      width: 640px;
      height: 480px;
      position: relative;
    }

    .layer {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

    #pose-video {
      transform: scaleX(-1);
    }

    #pose-result {
      font-size: 100px;
      text-align: right;
      padding: 20px 30px 0 0;
    }
    </style>
  </head>
  <body>
    <div id="video-container">
      <video id="pose-video" class="layer" playsinline></video>
      <canvas id="pose-canvas" class="layer"></canvas>
      <div id="pose-result" class="layer"></div>
    </div>
  
    <script>
  
      const config = {
        video: { width: 640, height: 480, fps: 30 }
      };
  
      const landmarkColors = {
        thumb: 'red',
        indexFinger: 'blue',
        middleFinger: 'yellow',
        ringFinger: 'green',
        pinky: 'pink',
        palmBase: 'white'
      };
  
      const gestureStrings = {
        'thumbs_up': '👍',
        'victory': '✌🏻'
      };
  
      async function main() {
  
        const video = document.querySelector("#pose-video");
        const canvas = document.querySelector("#pose-canvas");
        const ctx = canvas.getContext("2d");
  
        const resultLayer = document.querySelector("#pose-result");
  
        // configure gesture estimator
        // add "✌🏻" and "👍" as sample gestures
        const knownGestures = [
          fp.Gestures.VictoryGesture,
          fp.Gestures.ThumbsUpGesture
        ];
        const GE = new fp.GestureEstimator(knownGestures);
  
        // load handpose model
        const model = await handpose.load();
        console.log("Handpose model loaded");
  
        // main estimation loop
        const estimateHands = async () => {
  
          // clear canvas overlay
          ctx.clearRect(0, 0, config.video.width, config.video.height);
          resultLayer.innerText = '';
  
          // get hand landmarks from video
          // Note: Handpose currently only detects one hand at a time
          // Therefore the maximum number of predictions is 1
          const predictions = await model.estimateHands(video, true);
  
          for(let i = 0; i < predictions.length; i++) {
  
            // draw colored dots at each predicted joint position
            for(let part in predictions[i].annotations) {
              for(let point of predictions[i].annotations[part]) {
                drawPoint(ctx, point[0], point[1], 3, landmarkColors[part]);
              }
            }
  
            // now estimate gestures based on landmarks
            // using a minimum confidence of 7.5 (out of 10)
            const est = GE.estimate(predictions[i].landmarks, 7.5);
  
            if(est.gestures.length > 0) {
  
              // find gesture with highest confidence
              let result = est.gestures.reduce((p, c) => { 
                return (p.confidence > c.confidence) ? p : c;
              });
  
              resultLayer.innerText = gestureStrings[result.name];
            }
          }
  
          // ...and so on
          setTimeout(() => { estimateHands(); }, 1000 / config.video.fps);
        };
  
        estimateHands();
        console.log("Starting predictions");
      }
  
      async function initCamera(width, height, fps) {
  
        const constraints = {
          audio: false,
          video: {
            facingMode: "user",
            width: width,
            height: height,
            frameRate: { max: fps }
          }
        };
  
        const video = document.querySelector("#pose-video");
        video.width = width;
        video.height = height;
  
        // get video stream
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
  
        return new Promise(resolve => {
          video.onloadedmetadata = () => { resolve(video) };
        });
      }
  
      function drawPoint(ctx, x, y, r, color) {
        ctx.beginPath();
        ctx.arc(x, y, r, 0, 2 * Math.PI);
        ctx.fillStyle = color;
        ctx.fill();
      }
  
      window.addEventListener("DOMContentLoaded", () => {
  
        initCamera(
          config.video.width, config.video.height, config.video.fps
        ).then(video => {
          video.play();
          video.addEventListener("loadeddata", event => {
            console.log("Camera is ready");
            main();
          });
        });
  
        const canvas = document.querySelector("#pose-canvas");
        canvas.width = config.video.width;
        canvas.height = config.video.height;
        console.log("Canvas initialized");
      });
    </script>
    <a href="/">AirBoard</a><br>
    <form action="/newroom" id="new_room" method="GET">
        <input type="submit" value="새 회의">
    </form>
  </body>
</html>
